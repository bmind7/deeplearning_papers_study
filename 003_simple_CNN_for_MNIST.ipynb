{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on CNN paper [Convolutional neural networks for images, speech and time-series](https://www.researchgate.net/profile/Yann_Lecun/publication/2453996_Convolutional_Networks_for_Images_Speech_and_Time-Series/links/0deec519dfa2325502000000.pdf), Yann LeCun & Yoshua Bengio, 1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                                ])\n",
    "\n",
    "trainset = datasets.MNIST('./datasets/', download=True,\n",
    "                          train=True, transform=transform)\n",
    "valset = datasets.MNIST('./datasets', download=True,\n",
    "                        train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2411b6c1fa0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(CustomConv2d, self).__init__()\n",
    "\n",
    "        self.weigths = nn.Parameter(torch.rand(out_channels, in_channels, *kernel_size))\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return F.conv2d(input, self.weigths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModule, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            CustomConv2d(in_channels=1, out_channels=16, kernel_size=(5,5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            CustomConv2d(in_channels=16, out_channels=32, kernel_size=(5, 5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(32 * 4 * 4, 10)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        x = self.conv1(input)\n",
    "        x = self.conv2(x)\n",
    "        return self.out(x.view(x.size(0),-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create network instance and test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNModule(\n",
      "  (conv1): Sequential(\n",
      "    (0): CustomConv2d()\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): CustomConv2d()\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([64, 1, 28, 28])\n",
      "Output size: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "net = CNNModule()\n",
    "print(net)\n",
    "\n",
    "data_item = iter(train_loader).next()[0]\n",
    "print(data_item.size())\n",
    "output = net.forward(data_item)\n",
    "print(\"Output size:\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push network to GPU if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "\n",
    "if(torch.cuda.is_available):\n",
    "    torch.cuda.empty_cache()\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy and error validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(net, device, loader):\n",
    "    correct_count, all_count = 0, 0\n",
    "    for images, labels in loader:\n",
    "        labels = labels.to(device)\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = net.forward(images)\n",
    "            out = torch.argmax(out, dim=1)\n",
    "            correct_count += torch.sum(labels == out).item()\n",
    "\n",
    "        all_count += labels.size(0)\n",
    "\n",
    "    print(\"Number Of Images Tested =\", all_count)\n",
    "    print(\"Model Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:17<00:00, 52.26it/s, Loss: 42.46865809082028] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "Model Accuracy = 0.9368\n",
      "\n",
      "---Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 50.99it/s, Loss: 0.17770284509170492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "Model Accuracy = 0.9726\n",
      "\n",
      "---Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:19<00:00, 48.76it/s, Loss: 0.09575410001775053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "Model Accuracy = 0.9726\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "epochs = 3\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f'\\n---Epoch: {e}')\n",
    "    running_loss = 0.\n",
    "\n",
    "    pbar = tqdm(train_loader)\n",
    "    for images, labels in pbar:\n",
    "\n",
    "        labels = nn.functional.one_hot(labels, num_classes=10).float().to(device)\n",
    "        images = images.to(device)\n",
    "\n",
    "        out = net.forward(images)\n",
    "        loss = loss_func(out, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        pbar.set_postfix_str(f\"Loss: {running_loss/(pbar.last_print_n+1)}\")\n",
    "    \n",
    "    calc_acc(net, device, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n",
    "\n",
    "with torch.no_grad():\n",
    "    images = images.to(device)\n",
    "\n",
    "    out = net.forward(images)\n",
    "\n",
    "print(\"Prediction:\", torch.argmax(out[0]).item())    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cb0abe6a156ed333ad4fd07d9af989e8f53906384bd66e80c5bbb553a4eaba7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
