{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on CNN paper [Convolutional neural networks for images, speech and time-series](https://www.researchgate.net/profile/Yann_Lecun/publication/2453996_Convolutional_Networks_for_Images_Speech_and_Time-Series/links/0deec519dfa2325502000000.pdf), Yann LeCun & Yoshua Bengio, 1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                                ])\n",
    "\n",
    "trainset = datasets.MNIST('./datasets/', download=True,\n",
    "                          train=True, transform=transform)\n",
    "valset = datasets.MNIST('./datasets', download=True,\n",
    "                        train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2411b6c1fa0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(CustomConv2d, self).__init__()\n",
    "\n",
    "        self.weigths = nn.Parameter(torch.rand(out_channels, in_channels, *kernel_size))\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return F.conv2d(input, self.weigths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModule, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2,\n",
    "            ),\n",
    "            # CustomConv2d(in_channels=1, out_channels=16, kernel_size=(5,5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            # nn.Conv2d(16, 32, 5, 1, 2),\n",
    "            nn.Conv2d(16, 32, 5, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(32 * 5 * 5, 10)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        x = self.conv1(input)\n",
    "        x = self.conv2(x)\n",
    "        return self.out(x.view(x.size(0),-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create network instance and test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNModule(\n",
      "  (conv1): Sequential(\n",
      "    (0): CustomConv2d()\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=800, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([64, 1, 28, 28])\n",
      "Output size: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "net = CNNModule()\n",
    "print(net)\n",
    "\n",
    "data_item = iter(train_loader).next()[0]\n",
    "print(data_item.size())\n",
    "output = net.forward(data_item)\n",
    "print(\"Output size:\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push network to GPU if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "\n",
    "if(torch.cuda.is_available):\n",
    "    torch.cuda.empty_cache()\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy and error validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(net, device, loader):\n",
    "    correct_count, all_count = 0, 0\n",
    "    for images, labels in loader:\n",
    "        labels = labels.to(device)\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = net.forward(images)\n",
    "            out = torch.argmax(out, dim=1)\n",
    "            correct_count += torch.sum(labels == out).item()\n",
    "\n",
    "        all_count += labels.size(0)\n",
    "\n",
    "    print(\"Number Of Images Tested =\", all_count)\n",
    "    print(\"Model Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:19<00:00, 48.90it/s, Loss: 0.10506768055331152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "Model Accuracy = 0.9707\n",
      "\n",
      "---Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 430/938 [00:09<00:11, 45.36it/s, Loss: 0.09722725282361327]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15784/4101644050.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \"\"\"\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'1'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m255\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     \u001b[1;31m# put it from HWC to CHW format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "epochs = 3\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f'\\n---Epoch: {e}')\n",
    "    running_loss = 0.\n",
    "\n",
    "    pbar = tqdm(train_loader)\n",
    "    for images, labels in pbar:\n",
    "\n",
    "        labels = nn.functional.one_hot(labels, num_classes=10).float().to(device)\n",
    "        images = images.to(device)\n",
    "\n",
    "        out = net.forward(images)\n",
    "        loss = loss_func(out, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        pbar.set_postfix_str(f\"Loss: {running_loss/(pbar.last_print_n+1)}\")\n",
    "    \n",
    "    calc_acc(net, device, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n",
    "\n",
    "with torch.no_grad():\n",
    "    images = images.to(device)\n",
    "\n",
    "    out = net.forward(images)\n",
    "\n",
    "print(\"Prediction:\", torch.argmax(out[0]).item())    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cb0abe6a156ed333ad4fd07d9af989e8f53906384bd66e80c5bbb553a4eaba7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
