{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on CNN paper [Convolutional neural networks for images, speech and time-series](https://www.researchgate.net/profile/Yann_Lecun/publication/2453996_Convolutional_Networks_for_Images_Speech_and_Time-Series/links/0deec519dfa2325502000000.pdf), Yann LeCun & Yoshua Bengio, 1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                                ])\n",
    "\n",
    "trainset = datasets.MNIST('./datasets/', download=True,\n",
    "                          train=True, transform=transform)\n",
    "valset = datasets.MNIST('./datasets', download=True,\n",
    "                        train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x294ea4fd700>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(CustomConv2d, self).__init__()\n",
    "\n",
    "        self.weigths = nn.Parameter(torch.rand(\n",
    "            out_channels, in_channels, *kernel_size))\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return F.conv2d(input, self.weigths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create custom convolution function just as proof of concept, there is no sense to use it because it's incredibly slow. To make it fast it should be implemented on GPU or with SIMD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1., 1., 0., 1.],\n",
      "          [0., 1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 0., 0., 1.],\n",
      "          [1., 1., 1., 1., 0., 1.],\n",
      "          [0., 1., 0., 1., 1., 0.],\n",
      "          [0., 1., 0., 1., 0., 1.]]]])\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 1., 1.]]]])\n",
      "tensor([[[[2., 4., 4., 3., 4., 2.],\n",
      "          [5., 6., 5., 4., 4., 3.],\n",
      "          [4., 6., 6., 4., 5., 3.],\n",
      "          [4., 5., 5., 3., 5., 2.],\n",
      "          [4., 4., 7., 4., 5., 3.],\n",
      "          [2., 1., 4., 2., 4., 1.]]]])\n",
      "tensor([[[[True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True]]]])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "m_numpy = np.random.choice([0, 1], p=(0.5, 0.5), size=(1, 1, 6, 6))\n",
    "m = torch.from_numpy(m_numpy).type(torch.FloatTensor)\n",
    "\n",
    "print(m)\n",
    "\n",
    "weights = torch.tensor([[[[1., 1., 1.],\n",
    "                          [1., 0., 1.],\n",
    "                          [1., 1., 1.]]]])\n",
    "\n",
    "print(weights)\n",
    "\n",
    "output = F.conv2d(m, weights, padding=1)\n",
    "\n",
    "print(output)\n",
    "\n",
    "def conv2d(input: Tensor, weights: Tensor) -> Tensor:\n",
    "    output = torch.zeros(input.size(0), weights.size(0),\n",
    "                         input.size(2), input.size(3))\n",
    "\n",
    "    y_kernel_offset = int(weights.size(2) / 2)\n",
    "    x_kernel_offset = int(weights.size(3) / 2)\n",
    "\n",
    "    for batch_idx in range(output.size(0)):\n",
    "        for out_channel_idx in range(output.size(1)):\n",
    "            for y in range(output.size(2)):\n",
    "                for x in range(output.size(3)):\n",
    "                    for in_channel_idx in range(input.size(1)):\n",
    "                        for kernel_y_idx in range(-y_kernel_offset, weights.size(2)-y_kernel_offset):\n",
    "                            for kernel_x_idx in range(-x_kernel_offset, weights.size(3)-x_kernel_offset):\n",
    "                                if (y+kernel_y_idx) < 0 or (x+kernel_x_idx) < 0 or (y+kernel_y_idx) >= output.size(2) or (x+kernel_x_idx) >= output.size(3):\n",
    "                                    continue\n",
    "                                \n",
    "                                output[batch_idx][out_channel_idx][y][x] += \\\n",
    "                                    weights[out_channel_idx][in_channel_idx][kernel_y_idx+y_kernel_offset][kernel_x_idx+x_kernel_offset] * \\\n",
    "                                    input[batch_idx][in_channel_idx][y+kernel_y_idx][x+kernel_x_idx]\n",
    "\n",
    "    return output\n",
    "\n",
    "output2 = conv2d(m, weights)\n",
    "print(output == output2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModule, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            CustomConv2d(in_channels=1, out_channels=16, kernel_size=(5,5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            CustomConv2d(in_channels=16, out_channels=32, kernel_size=(5, 5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(32 * 4 * 4, 10)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        x = self.conv1(input)\n",
    "        x = self.conv2(x)\n",
    "        return self.out(x.view(x.size(0),-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create network instance and test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNModule(\n",
      "  (conv1): Sequential(\n",
      "    (0): CustomConv2d()\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): CustomConv2d()\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([64, 1, 28, 28])\n",
      "Output size: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "net = CNNModule()\n",
    "print(net)\n",
    "\n",
    "data_item = iter(train_loader).next()[0]\n",
    "print(data_item.size())\n",
    "output = net.forward(data_item)\n",
    "print(\"Output size:\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push network to GPU if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "\n",
    "if(torch.cuda.is_available):\n",
    "    torch.cuda.empty_cache()\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy and error validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(net, device, loader):\n",
    "    correct_count, all_count = 0, 0\n",
    "    for images, labels in loader:\n",
    "        labels = labels.to(device)\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = net.forward(images)\n",
    "            out = torch.argmax(out, dim=1)\n",
    "            correct_count += torch.sum(labels == out).item()\n",
    "\n",
    "        all_count += labels.size(0)\n",
    "\n",
    "    print(\"Number Of Images Tested =\", all_count)\n",
    "    print(\"Model Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:25<00:00, 36.30it/s, Loss: 34.94965296758293] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "Model Accuracy = 0.9382\n",
      "\n",
      "---Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:17<00:00, 52.61it/s, Loss: 0.1751946564089998] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "Model Accuracy = 0.9694\n",
      "\n",
      "---Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 50.70it/s, Loss: 0.09405055476192718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "Model Accuracy = 0.9792\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "epochs = 3\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f'\\n---Epoch: {e}')\n",
    "    running_loss = 0.\n",
    "\n",
    "    pbar = tqdm(train_loader)\n",
    "    for images, labels in pbar:\n",
    "\n",
    "        labels = nn.functional.one_hot(labels, num_classes=10).float().to(device)\n",
    "        images = images.to(device)\n",
    "\n",
    "        out = net.forward(images)\n",
    "        loss = loss_func(out, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        pbar.set_postfix_str(f\"Loss: {running_loss/(pbar.last_print_n+1)}\")\n",
    "    \n",
    "    calc_acc(net, device, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n",
    "\n",
    "with torch.no_grad():\n",
    "    images = images.to(device)\n",
    "\n",
    "    out = net.forward(images)\n",
    "\n",
    "print(\"Prediction:\", torch.argmax(out[0]).item())    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cb0abe6a156ed333ad4fd07d9af989e8f53906384bd66e80c5bbb553a4eaba7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
