{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study of paper [A learning algorithm for continually running fully recurrent neural networks 1989](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.9724&rep=rep1&type=pdf)\n",
    "\n",
    "Storing long string of text into 30 neurons RNN. It's not an attempt of storing data in efficient way, just a study of RNNs in general.\n",
    "\n",
    "Text used is Churchill's \"We shall fight on the beaches\" speech:\n",
    "\n",
    "<blockquote>\n",
    "The British Empire and the French Republic, linked together in their cause and in their need, will defend to the death their native soil, aiding each other like good comrades to the utmost of their strength.\n",
    "\n",
    "Even though large tracts of Europe and many old and famous states have fallen or may fall into the grip of the Gestapo and all the odious apparatus of Nazi rule, we shall not flag or fail.\n",
    "\n",
    "We shall go on to the end, we shall fight in France, we shall fight on the seas and oceans, we shall fight with growing confidence and growing strength in the air, we shall defend our island, whatever the cost may be.\n",
    "\n",
    "We shall fight on the beaches, we shall fight on the landing grounds, we shall fight in the fields and in the streets, we shall fight in the hills; we shall never surrender, and even if, which I do not for a moment believe, this island or a large part of it were subjugated and starving, then our Empire beyond the seas, armed and guarded by the British fleet, would carry on the struggle, until, in God's good time, the new world, with all its power and might, steps forth to the rescue and the liberation of the old.\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "trainset = [['', 'The British Empire and the French Republic, linked together in their cause and in their need, will defend to the death their native soil, aiding each other like good comrades to the utmost of their strength. Even though large tracts of Europe and many old and famous states have fallen or may fall into the grip of the Gestapo and all the odious apparatus of Nazi rule, we shall not flag or fail. We shall go on to the end, we shall fight in France, we shall fight on the seas and oceans, we shall fight with growing confidence and growing strength in the air, we shall defend our island, whatever the cost may be. We shall fight on the beaches, we shall fight on the landing grounds, we shall fight in the fields and in the streets, we shall fight in the hills; we shall never surrender, and even if, which I do not for a moment believe, this island or a large part of it were subjugated and starving, then our Empire beyond the seas, armed and guarded by the British fleet, would carry on the struggle, until, in Gods good time, the new world, with all its power and might, steps forth to the rescue and the liberation of the old.']]\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset: \n",
    "* remove html tags\n",
    "* convert to ascii\n",
    "* character embedding \n",
    "\n",
    "_Note: supports batch size 1 only_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset):\n",
    "    _, review = dataset[0][0], dataset[1][0]\n",
    "    # 1 indicates batch size, we support only one entry per batch\n",
    "    # 128 is amount ascii simbols for one hot encoding\n",
    "    output_review = torch.zeros(len(review)-1, 1, 128)\n",
    "    output_labels = torch.zeros(len(review)-1, 1, 128)\n",
    "\n",
    "    # clean up data\n",
    "    data_processed = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    data_processed = data_processed.encode('ascii', 'ignore')\n",
    "\n",
    "    # create one hot char encoding\n",
    "    for idx in range(len(data_processed)-1):\n",
    "        in_char = data_processed[idx]\n",
    "        out_char = data_processed[idx+1]\n",
    "        output_review[idx][0][in_char] = 1.\n",
    "        output_labels[idx][0][out_char] = 1.\n",
    "\n",
    "    return [output_review, output_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, layers_number):\n",
    "        super(Module, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers_number = layers_number\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, layers_number)\n",
    "        self.out_layer = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, input, hidden_state = None):\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        if hidden_state == None:\n",
    "            # find out at what device model is located\n",
    "            device = next(self.parameters()).device\n",
    "            hidden_state = torch.zeros(self.layers_number, batch_size, self.hidden_dim).to(device)\n",
    "\n",
    "        out, hidden_state = self.rnn(input, hidden_state)\n",
    "\n",
    "        out = self.out_layer(out)\n",
    "\n",
    "        # out = F.softmax(out, dim=2)\n",
    "\n",
    "        return out, hidden_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model and make a test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(\n",
      "  (rnn): RNN(128, 32)\n",
      "  (out_layer): Linear(in_features=32, out_features=128, bias=True)\n",
      ")\n",
      "torch.Size([1131, 1, 128])\n",
      "torch.Size([1131, 1, 128])\n",
      "tensor(4.8765, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "net = Module(input_size=128, output_size=128, hidden_dim=32, layers_number=1)\n",
    "print(net)\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "prepared_data = process_dataset(data_iter.next())\n",
    "out, hidden = net.forward(prepared_data[0])\n",
    "print(prepared_data[1].size())\n",
    "print(out.size())\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss = loss_func(out.view(-1, 128), prepared_data[1].view(-1, 128))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate if we can use GPU and set it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "\n",
    "if(torch.cuda.is_available):\n",
    "    torch.cuda.empty_cache()\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(model: Module, length, start_char) -> None:\n",
    "    output = [start_char]\n",
    "    input_char = start_char.encode(\"ascii\")[0]\n",
    "    input_emb = torch.zeros(1, 1, 128).to(device)\n",
    "    input_emb[0][0][input_char] = 1\n",
    "    hidden_state = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            input_emb, hidden_state = model.forward(input_emb, hidden_state=hidden_state)\n",
    "            char_idx = torch.argmax(input_emb).item()\n",
    "            output.append(chr(char_idx))\n",
    "            input_emb = torch.zeros(1, 1, 128).to(device)\n",
    "            input_emb[0][0][char_idx] = 1\n",
    "\n",
    "    print(f\"Generated output: {''.join(output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: The British Empire and the French Republic, linked together in their cause and in their need, will defend to the death their native soil, aiding each other like good comrades to the utmost of their strength. Even though large tracts of Europe and many old and famous states have fallen or may fall into the grip of the Gestapo and all the odious apparatus of Nazi rule, we shall not flag or fail. We shall go on to the end, we shall fight in France, we shall fight on the seas and oceans, we shall fight with growing confidence and growing strength in the air, we shall defend our island, whatever the cost may be. We shall fight on the beaches, we shall fight on the landing grounds, we shall fight in the fields and in the streets, we shall fight in the hills; we shall never surrender, and even if, which I do not for a moment believe, this island or a large part of it were subjugated and starving, then our Empire beyond the seas, armed and guarded by the British fleet, would carry on the struggle, until, in Gods good time, the new world, with all its power and might, steps forth to the rescue and the liberation of the old.\n"
     ]
    }
   ],
   "source": [
    "generate_sequence(net, 1131, 'T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9408"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in net.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:00<03:17,  5.05it/s, Loss: 0.2888025641441345] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: The British Empire and the Frend fatm belile, aist fountoug aralledengos of Eveyt. Eurep and in firhi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 102/1000 [00:11<01:44,  8.58it/s, Loss: 0.08720751851797104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: The British Empire and the French Republic, linked together in their cause and in their need, will de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 202/1000 [00:23<01:41,  7.86it/s, Loss: 0.07222181558609009]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: The British Empire and the French Republic, linked together in their cause and in their need, will de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 302/1000 [00:35<01:25,  8.12it/s, Loss: 0.06023353710770607]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: The British Empire and the French Republic, linked together in their cause and in their need, will de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 401/1000 [00:46<01:16,  7.79it/s, Loss: 0.05705444887280464]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: The British Empire and the French Republic, linked together in their cause and in their need, will de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 501/1000 [00:58<01:12,  6.89it/s, Loss: 0.05484971031546593] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: The British Empire and the French Republic, linked together in their cause and in their need, will de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 602/1000 [01:10<00:50,  7.82it/s, Loss: 0.053294263780117035]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: The British Empire and the French Republic, linked together in their cause and in their need, will de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 701/1000 [01:22<00:41,  7.14it/s, Loss: 0.05314226076006889]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: The British Empire and the French Republic, linked together in their cause and in their need, will de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 802/1000 [01:35<00:27,  7.28it/s, Loss: 0.05092659592628479]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: The British Empire and the French Republic, linked together in their cause and in their need, will de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 902/1000 [01:47<00:12,  7.87it/s, Loss: 0.0497162789106369]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: The British Empire and the French Republic, linked together in their cause and in their need, will de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:58<00:00,  8.44it/s, Loss: 0.04854488745331764]\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0002)\n",
    "epochs = 1000\n",
    "acc = 0\n",
    "data_items_in_epoch = 100\n",
    "\n",
    "pbar = tqdm(range(epochs))\n",
    "for e in pbar:\n",
    "    running_loss = 0.\n",
    "\n",
    "    for data_item in train_loader:\n",
    "\n",
    "        reviews, labels = process_dataset(data_item)\n",
    "        labels = labels.to(device)\n",
    "        reviews = reviews.to(device)\n",
    "\n",
    "        out, _ = net.forward(reviews)\n",
    "        loss = loss_func(out.view(-1, 128), labels.view(-1, 128))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    pbar.set_postfix_str(f\"Loss: {running_loss}\")\n",
    "    if(e % 100 == 0):\n",
    "        generate_sequence(net, 100, \"T\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cb0abe6a156ed333ad4fd07d9af989e8f53906384bd66e80c5bbb553a4eaba7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
